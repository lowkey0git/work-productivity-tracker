from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import os
import warnings
import uvicorn
import logging
from contextlib import asynccontextmanager

warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global variable to store the tracker instance
blog_tracker = None


class BlogPostTracker:
    def __init__(self, file_path="blog_data.csv"):
        self.file_path = file_path
        self.scaler = MinMaxScaler()
        self.label_encoder = LabelEncoder()
        self.pca = PCA(n_components=10)
        self.best_model = None
        self.best_model_name = "RandomForest"
        self.feature_columns = [
            'posting_delay', 'early_posting', 'writing_duration',
            'scheduled_duration', 'duration_difference', 'posting_hour',
            'posting_day_of_week', 'posting_month'
        ]
        self.dataframe_encoded_columns = None
        self.results = []
        self.is_trained = False

    def load_data(self):
        """Load CSV file from the specified path with validation"""
        try:
            df = pd.read_csv(self.file_path)

            # Validate required columns
            required_columns = ['start_writing', 'post_published', 'scheduled_start', 'scheduled_deadline']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise Exception(f"Missing required columns: {missing_columns}")

            logger.info(f"Successfully loaded data with shape: {df.shape}")
            return df

        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            raise Exception(f"Error loading data: {str(e)}")

    def preprocess_data(self, df):
        try:
            # Convert time columns to datetime with error handling
            datetime_columns = ['start_writing', 'post_published', 'scheduled_start', 'scheduled_deadline']
            for col in datetime_columns:
                try:
                    df[col] = pd.to_datetime(df[col])
                except Exception as e:
                    raise Exception(f"Error parsing datetime column '{col}': {str(e)}")

            # Feature engineering with improved calculations
            df['posting_delay'] = np.maximum(0,
                                             (df['post_published'] - df['scheduled_deadline']).dt.total_seconds() / 60)
            df['early_posting'] = np.maximum(0,
                                             (df['scheduled_deadline'] - df['post_published']).dt.total_seconds() / 60)
            df['writing_duration'] = (df['post_published'] - df['start_writing']).dt.total_seconds() / 3600
            df['scheduled_duration'] = (df['scheduled_deadline'] - df['scheduled_start']).dt.total_seconds() / 3600
            df['duration_difference'] = df['writing_duration'] - df['scheduled_duration']
            df['posting_hour'] = df['post_published'].dt.hour
            df['posting_day_of_week'] = df['post_published'].dt.dayofweek
            df['posting_month'] = df['post_published'].dt.month

            # Create posting performance status with improved logic
            def categorize_posting_performance(row):
                # On time: within 30 minutes early or late
                if row['posting_delay'] <= 30 and row['early_posting'] <= 30:
                    return 'on_schedule'
                # Poor timing: more than 4 hours late or 8 hours early
                elif row['posting_delay'] > 240 or row['early_posting'] > 480:
                    return 'poor_timing'
                else:
                    return 'acceptable'

            df['posting_status'] = df.apply(categorize_posting_performance, axis=1)

            # Validate that we have reasonable feature values
            for col in self.feature_columns:
                if col in df.columns:
                    if df[col].isnull().sum() > 0:
                        logger.warning(f"Found {df[col].isnull().sum()} null values in {col}")

            logger.info(
                f"Preprocessing completed. Status distribution: {df['posting_status'].value_counts().to_dict()}")
            return df

        except Exception as e:
            logger.error(f"Error preprocessing data: {str(e)}")
            raise Exception(f"Error preprocessing data: {str(e)}")

    def train_models(self):
        """Train RandomForest model with GridSearchCV for best parameters"""
        try:
            logger.info("Starting model training with RandomForest and GridSearchCV...")
            df = self.load_data()
            df = self.preprocess_data(df)

            # Prepare features
            dataframe = df[self.feature_columns + ['posting_status']].dropna()

            # Enhanced data size validation
            if len(dataframe) < 20:
                raise Exception("Not enough data to train models reliably. Need at least 20 records.")

            # Check class distribution
            status_counts = dataframe['posting_status'].value_counts()
            logger.info(f"Class distribution: {status_counts.to_dict()}")

            if len(status_counts) < 2:
                raise Exception("Need at least 2 different posting status categories to train models.")

            # Check for classes with very few samples
            min_class_size = status_counts.min()
            if min_class_size < 3:
                logger.warning(
                    f"Some classes have very few samples (min: {min_class_size}). This may affect model performance.")

            dataframe_encoded = pd.get_dummies(dataframe.drop(['posting_status'], axis=1), drop_first=True)
            self.dataframe_encoded_columns = dataframe_encoded.columns.tolist()

            # Scale features
            scaled_features = self.scaler.fit_transform(dataframe_encoded)

            # Encode target
            X = scaled_features
            Y = self.label_encoder.fit_transform(dataframe['posting_status'])

            # Split data with stratification when possible
            try:
                X_train, X_test, y_train, y_test = train_test_split(
                    X, Y, test_size=0.2, random_state=42, stratify=Y
                )
            except ValueError:
                # If stratification fails due to small sample sizes, split without stratification
                logger.warning("Stratified split failed, using random split")
                X_train, X_test, y_train, y_test = train_test_split(
                    X, Y, test_size=0.2, random_state=42
                )

            # Apply PCA with better component calculation
            n_features = X_train.shape[1]
            n_samples = X_train.shape[0]
            n_components = min(10, n_features, max(1, n_samples // 2))
            self.pca = PCA(n_components=n_components)

            logger.info(f"Using {n_components} PCA components from {n_features} features")

            X_train_pca = self.pca.fit_transform(X_train)
            X_test_pca = self.pca.transform(X_test)

            # Define parameter grid for RandomForest
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 5, 7, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None]
            }

            # Create RandomForest classifier
            rf = RandomForestClassifier(random_state=42)

            # Perform GridSearchCV
            logger.info("Performing GridSearchCV to find best parameters...")
            grid_search = GridSearchCV(
                estimator=rf,
                param_grid=param_grid,
                cv=3,  # 3-fold cross-validation
                scoring='accuracy',
                n_jobs=-1,  # Use all available cores
                verbose=1
            )

            # Fit GridSearchCV
            grid_search.fit(X_train_pca, y_train)

            # Get best model
            self.best_model = grid_search.best_estimator_

            # Evaluate on test set
            y_pred = self.best_model.predict(X_test_pca)
            accuracy = accuracy_score(y_test, y_pred)

            # Store results
            result = {
                'model': self.best_model_name,
                'best_parameters': grid_search.best_params_,
                'best_cv_score': grid_search.best_score_,
                'test_accuracy': accuracy,
                'train_samples': len(X_train_pca),
                'test_samples': len(X_test_pca)
            }

            self.results = [result]
            self.is_trained = True

            logger.info(f"GridSearchCV completed!")
            logger.info(f"Best parameters: {grid_search.best_params_}")
            logger.info(f"Best CV score: {grid_search.best_score_:.4f}")
            logger.info(f"Test accuracy: {accuracy:.4f}")

            return self.results

        except Exception as e:
            logger.error(f"Error training model: {str(e)}")
            raise Exception(f"Error training model: {str(e)}")

    def predict_posting_performance(self, blogger_data):
        """Predict posting performance for new blogger data"""
        if not self.is_trained or self.best_model is None:
            raise ValueError("Model not trained yet. Call train_models() first.")

        try:
            # Process new data
            new_df = pd.DataFrame([blogger_data])

            # Apply same preprocessing as training data
            new_df = self.preprocess_data(new_df)

            # Select features
            new_features = new_df[self.feature_columns]

            # Handle categorical encoding more robustly
            new_features_encoded = pd.get_dummies(new_features, drop_first=True)

            # Ensure exact column matching with training data
            for col in self.dataframe_encoded_columns:
                if col not in new_features_encoded.columns:
                    new_features_encoded[col] = 0

            # Remove extra columns and reorder to match training
            new_features_encoded = new_features_encoded[self.dataframe_encoded_columns]

            # Validate shape before scaling
            if new_features_encoded.shape[1] != len(self.dataframe_encoded_columns):
                raise ValueError(
                    f"Feature shape mismatch. Expected {len(self.dataframe_encoded_columns)} features, got {new_features_encoded.shape[1]}")

            # Scale and apply PCA
            new_features_scaled = self.scaler.transform(new_features_encoded)
            new_features_pca = self.pca.transform(new_features_scaled)

            # Predict
            prediction = self.best_model.predict(new_features_pca)[0]

            # Get probabilities
            probability = self.best_model.predict_proba(new_features_pca)[0]
            prob_dict = dict(zip(self.label_encoder.classes_, probability))

            result = {
                'predicted_status': self.label_encoder.inverse_transform([prediction])[0],
                'probabilities': prob_dict,
                'model_used': self.best_model_name
            }

            logger.info(f"Prediction made: {result['predicted_status']}")
            return result

        except Exception as e:
            logger.error(f"Error making prediction: {str(e)}")
            raise Exception(f"Error making prediction: {str(e)}")

    def get_posts_in_timeframe(self, start_time: datetime, end_time: datetime):
        """Get blog posts within a specific timeframe"""
        try:
            df = self.load_data()
            df = self.preprocess_data(df)

            # Filter by timeframe
            mask = (df['post_published'] >= start_time) & (df['post_published'] <= end_time)
            filtered_df = df[mask]

            result = {
                'total_posts': len(filtered_df),
                'posts_by_status': filtered_df['posting_status'].value_counts().to_dict(),
                'posts_data': filtered_df.to_dict('records')
            }

            logger.info(f"Retrieved {result['total_posts']} posts from timeframe {start_time} to {end_time}")
            return result

        except Exception as e:
            logger.error(f"Error querying posts in timeframe: {str(e)}")
            raise Exception(f"Error querying posts in timeframe: {str(e)}")


# Pydantic models for API requests/responses
class BlogPostPredictionRequest(BaseModel):
    start_writing: datetime = Field(..., description="When the blogger started writing")
    post_published: datetime = Field(..., description="When the post was published")
    scheduled_start: datetime = Field(..., description="When the blogger was scheduled to start")
    scheduled_deadline: datetime = Field(..., description="When the post was scheduled to be published")


class TimeframeRequest(BaseModel):
    start_time: datetime = Field(..., description="Start time for the query")
    end_time: datetime = Field(..., description="End time for the query")


class PredictionResponse(BaseModel):
    predicted_status: str
    probabilities: Optional[Dict[str, float]]
    model_used: str


class TimeframeResponse(BaseModel):
    total_posts: int
    posts_by_status: Dict[str, int]
    posts_data: List[Dict[str, Any]]


class TrainingResponse(BaseModel):
    message: str
    model_performance: List[Dict[str, Any]]
    best_model: str


async def startup_event():
    """Initialize the blog tracker on startup"""
    global blog_tracker
    logger.info("Initializing Blog Post Tracker...")
    blog_tracker = BlogPostTracker()
    try:
        # Try to train the model on startup if data is available
        blog_tracker.train_models()
        logger.info("✅ Model trained successfully on startup")
    except Exception as e:
        logger.warning(f"⚠️  Could not train model on startup: {e}")
        logger.info("📝 Model will need to be trained via /train endpoint")


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await startup_event()
    yield
    # Shutdown
    logger.info("Shutting down Blog Post Tracker...")


# Create FastAPI app
app = FastAPI(
    title="Blog Post Performance Tracker API",
    description="API for tracking and predicting blog post performance",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/")
async def root():
    """Root endpoint with API information"""
    return {
        "message": "Blog Post Performance Tracker API",
        "version": "1.0.0",
        "endpoints": {
            "POST /predict": "Predict blog post performance",
            "POST /posts/timeframe": "Get posts within timeframe",
            "POST /train": "Train the ML models",
            "GET /health": "Health check",
            "GET /model/performance": "Get model performance metrics",
            "GET /debug/data-info": "Debug data information"
        }
    }


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    global blog_tracker
    return {
        "status": "healthy",
        "model_trained": blog_tracker.is_trained if blog_tracker else False,
        "timestamp": datetime.now().isoformat()
    }


@app.get("/debug/data-info")
async def debug_data_info():
    """Debug endpoint to check data status"""
    global blog_tracker

    if not blog_tracker:
        return {"error": "Blog tracker not initialized"}

    try:
        df = blog_tracker.load_data()
        preprocessed_df = blog_tracker.preprocess_data(df)

        return {
            "data_shape": df.shape,
            "columns": df.columns.tolist(),
            "data_types": df.dtypes.to_dict(),
            "first_few_rows": df.head(3).to_dict('records'),
            "null_counts": df.isnull().sum().to_dict(),
            "preprocessed_shape": preprocessed_df.shape,
            "status_distribution": preprocessed_df['posting_status'].value_counts().to_dict(),
            "feature_stats": preprocessed_df[blog_tracker.feature_columns].describe().to_dict()
        }
    except Exception as e:
        logger.error(f"Debug info error: {str(e)}")
        return {"error": str(e)}


@app.post("/train", response_model=TrainingResponse)
async def train_model():
    """Train the ML models"""
    global blog_tracker

    if not blog_tracker:
        raise HTTPException(status_code=500, detail="Blog tracker not initialized")

    try:
        results = blog_tracker.train_models()
        return TrainingResponse(
            message="Model trained successfully",
            model_performance=results,
            best_model=blog_tracker.best_model_name
        )
    except Exception as e:
        logger.error(f"Training failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Training failed: {str(e)}")


@app.post("/predict", response_model=PredictionResponse)
async def predict_performance(request: BlogPostPredictionRequest):
    """Predict blog post performance based on timing data"""
    global blog_tracker

    if not blog_tracker:
        logger.error("Blog tracker not initialized")
        raise HTTPException(status_code=500, detail="Blog tracker not initialized")

    if not blog_tracker.is_trained:
        logger.warning("Prediction attempted on untrained model")
        raise HTTPException(status_code=400, detail="Model not trained. Please call /train endpoint first.")

    # Validate datetime inputs
    if request.start_writing >= request.post_published:
        raise HTTPException(status_code=400, detail="start_writing must be before post_published")

    if request.scheduled_start >= request.scheduled_deadline:
        raise HTTPException(status_code=400, detail="scheduled_start must be before scheduled_deadline")

    try:
        # Convert request to dictionary
        blogger_data = {
            'start_writing': request.start_writing.isoformat(),
            'post_published': request.post_published.isoformat(),
            'scheduled_start': request.scheduled_start.isoformat(),
            'scheduled_deadline': request.scheduled_deadline.isoformat()
        }

        result = blog_tracker.predict_posting_performance(blogger_data)

        return PredictionResponse(
            predicted_status=result['predicted_status'],
            probabilities=result['probabilities'],
            model_used=result['model_used']
        )
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")


@app.post("/posts/timeframe", response_model=TimeframeResponse)
async def get_posts_timeframe(request: TimeframeRequest):
    """Get blog posts within a specific timeframe"""
    global blog_tracker

    if not blog_tracker:
        raise HTTPException(status_code=500, detail="Blog tracker not initialized")

    # Validate timeframe
    if request.start_time >= request.end_time:
        raise HTTPException(status_code=400, detail="start_time must be before end_time")

    try:
        result = blog_tracker.get_posts_in_timeframe(request.start_time, request.end_time)

        return TimeframeResponse(
            total_posts=result['total_posts'],
            posts_by_status=result['posts_by_status'],
            posts_data=result['posts_data']
        )
    except Exception as e:
        logger.error(f"Timeframe query failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Query failed: {str(e)}")


@app.get("/model/performance")
async def get_model_performance():
    """Get performance metrics of trained models"""
    global blog_tracker

    if not blog_tracker:
        raise HTTPException(status_code=500, detail="Blog tracker not initialized")

    if not blog_tracker.is_trained:
        raise HTTPException(status_code=400, detail="Model not trained. Please call /train endpoint first.")

    return {
        "model_performance": blog_tracker.results,
        "best_model": blog_tracker.best_model_name,
        "feature_columns": blog_tracker.feature_columns,
        "is_trained": blog_tracker.is_trained,
        "total_features_after_encoding": len(
            blog_tracker.dataframe_encoded_columns) if blog_tracker.dataframe_encoded_columns else 0
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
