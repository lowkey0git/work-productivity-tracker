from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import os
import warnings
import uvicorn
import logging
from contextlib import asynccontextmanager

warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global variable to store the tracker instance
blog_tracker = None


class BlogPostTracker:
    def __init__(self, file_path="blog_data.csv"):
        self.file_path = file_path
        self.scaler = MinMaxScaler()
        self.label_encoder = LabelEncoder()
        self.pca = PCA(n_components=10)
        self.best_model = None
        self.best_model_name = "RandomForest"
        self.feature_columns = [
            'posting_delay', 'early_posting', 'writing_duration',
            'scheduled_duration', 'duration_difference', 'posting_hour',
            'posting_day_of_week', 'posting_month'
        ]
        self.dataframe_encoded_columns = None
        self.results = []
        self.is_trained = False

    def load_data(self):
        """Load CSV file from the specified path with validation"""
        try:
            df = pd.read_csv(self.file_path)

            # Validate required columns
            required_columns = ['start_writing', 'post_published', 'scheduled_start', 'scheduled_deadline']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise Exception(f"Missing required columns: {missing_columns}")

            logger.info(f"Successfully loaded data with shape: {df.shape}")
            return df

        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            raise Exception(f"Error loading data: {str(e)}")

    def preprocess_data(self, df):
        try:
            # Convert time columns to datetime with error handling
            datetime_columns = ['start_writing', 'post_published', 'scheduled_start', 'scheduled_deadline']
            for col in datetime_columns:
                try:
                    df[col] = pd.to_datetime(df[col])
                except Exception as e:
                    raise Exception(f"Error parsing datetime column '{col}': {str(e)}")

            # Feature engineering with improved calculations
            df['posting_delay'] = np.maximum(0,
                                             (df['post_published'] - df['scheduled_deadline']).dt.total_seconds() / 60)
            df['early_posting'] = np.maximum(0,
                                             (df['scheduled_deadline'] - df['post_published']).dt.total_seconds() / 60)
            df['writing_duration'] = (df['post_published'] - df['start_writing']).dt.total_seconds() / 3600
            df['scheduled_duration'] = (df['scheduled_deadline'] - df['scheduled_start']).dt.total_seconds() / 3600
            df['duration_difference'] = df['writing_duration'] - df['scheduled_duration']
            df['posting_hour'] = df['post_published'].dt.hour
            df['posting_day_of_week'] = df['post_published'].dt.dayofweek
            df['posting_month'] = df['post_published'].dt.month

            # Create posting performance status with improved logic
            def categorize_posting_performance(row):
                # On time: within 30 minutes early or late
                if row['posting_delay'] <= 30 and row['early_posting'] <= 30:
                    return 'on_schedule'
                # Poor timing: more than 4 hours late or 8 hours early
                elif row['posting_delay'] > 240 or row['early_posting'] > 480:
                    return 'poor_timing'
                else:
                    return 'acceptable'

            df['posting_status'] = df.apply(categorize_posting_performance, axis=1)

            # Validate that we have reasonable feature values
            for col in self.feature_columns:
                if col in df.columns:
                    if df[col].isnull().sum() > 0:
                        logger.warning(f"Found {df[col].isnull().sum()} null values in {col}")

            logger.info(
                f"Preprocessing completed. Status distribution: {df['posting_status'].value_counts().to_dict()}")
            return df

        except Exception as e:
            logger.error(f"Error preprocessing data: {str(e)}")
            raise Exception(f"Error preprocessing data: {str(e)}")

    def train_models(self):
        """Train RandomForest model with GridSearchCV for best parameters"""
        try:
            logger.info("Starting model training with RandomForest and GridSearchCV...")
            df = self.load_data()
            df = self.preprocess_data(df)

            # Prepare features
            dataframe = df[self.feature_columns + ['posting_status']].dropna()

            # Enhanced data size validation
            if len(dataframe) < 20:
                raise Exception("Not enough data to train models reliably. Need at least 20 records.")

            # Check class distribution
            status_counts = dataframe['posting_status'].value_counts()
            logger.info(f"Class distribution: {status_counts.to_dict()}")

            if len(status_counts) < 2:
                raise Exception("Need at least 2 different posting status categories to train models.")

            # Check for classes with very few samples
            min_class_size = status_counts.min()
            if min_class_size < 3:
                logger.warning(
                    f"Some classes have very few samples (min: {min_class_size}). This may affect model performance.")

            dataframe_encoded = pd.get_dummies(dataframe.drop(['posting_status'], axis=1), drop_first=True)
            self.dataframe_encoded_columns = dataframe_encoded.columns.tolist()

            # Scale features
            scaled_features = self.scaler.fit_transform(dataframe_encoded)

            # Encode target
            X = scaled_features
            Y = self.label_encoder.fit_transform(dataframe['posting_status'])

            # Split data with stratification when possible
            try:
                X_train, X_test, y_train, y_test = train_test_split(
                    X, Y, test_size=0.2, random_state=42, stratify=Y
                )
            except ValueError:
                # If stratification fails due to small sample sizes, split without stratification
                logger.warning("Stratified split failed, using random split")
                X_train, X_test, y_train, y_test = train_test_split(
                    X, Y, test_size=0.2, random_state=42
                )

            # Apply PCA with better component calculation
            n_features = X_train.shape[1]
            n_samples = X_train.shape[0]
            n_components = min(10, n_features, max(1, n_samples // 2))
            self.pca = PCA(n_components=n_components)

            logger.info(f"Using {n_components} PCA components from {n_features} features")

            X_train_pca = self.pca.fit_transform(X_train)
            X_test_pca = self.pca.transform(X_test)

            # Define parameter grid for RandomForest
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 5, 7, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None]
            }

            # Create RandomForest classifier
            rf = RandomForestClassifier(random_state=42)

            # Perform GridSearchCV
            logger.info("Performing GridSearchCV to find best parameters...")
            grid_search = GridSearchCV(
                estimator=rf,
                param_grid=param_grid,
                cv=3,  # 3-fold cross-validation
                scoring='accuracy',
                n_jobs=-1,  # Use all available cores
                verbose=1
            )

            # Fit GridSearchCV
            grid_search.fit(X_train_pca, y_train)

            # Get best model
            self.best_model = grid_search.best_estimator_

            # Evaluate on test set
            y_pred = self.best_model.predict(X_test_pca)
            accuracy = accuracy_score(y_test, y_pred)

            # Store results
            result = {
                'model': self.best_model_name,
                'best_parameters': grid_search.best_params_,
                'best_cv_score': grid_search.best_score_,
                'test_accuracy': accuracy,
                'train_samples': len(X_train_pca),
                'test_samples': len(X_test_pca)
            }

            self.results = [result]
            self.is_trained = True

            logger.info(f"GridSearchCV completed!")
            logger.info(f"Best parameters: {grid_search.best_params_}")
            logger.info(f"Best CV score: {grid_search.best_score_:.4f}")
            logger.info(f"Test accuracy: {accuracy:.4f}")

            return self.results

        except Exception as e:
            logger.error(f"Error training model: {str(e)}")
            raise Exception(f"Error training model: {str(e)}")


    def get_posts_in_timeframe(self, start_time: datetime, end_time: datetime):
        """Get blog posts within a specific timeframe"""
        try:
            df = self.load_data()
            df = self.preprocess_data(df)

            # Filter by timeframe
            mask = (df['post_published'] >= start_time) & (df['post_published'] <= end_time)
            filtered_df = df[mask]

            result = {
                'total_posts': len(filtered_df),
                'posts_by_status': filtered_df['posting_status'].value_counts().to_dict(),
                'posts_data': filtered_df.to_dict('records')
            }

            logger.info(f"Retrieved {result['total_posts']} posts from timeframe {start_time} to {end_time}")
            return result

        except Exception as e:
            logger.error(f"Error querying posts in timeframe: {str(e)}")
            raise Exception(f"Error querying posts in timeframe: {str(e)}")

